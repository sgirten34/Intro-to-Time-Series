---
title: "Models"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	fig.align = "center"
)
```

## Friday's Notes

## MA(q) Process

$\rho_k = \frac{-\theta_k + \theta_1 \theta_{k+1} + \theta_2 \theta_{k+2} + ...}{1 + \theta_1^2 + \theta_2^2 + ... + \theta_q^2}$ for k = 1, 2, ..., q

Or

$\rho = \frac{-theta_q}{1+\theta_1^2 + \theta_2^2 + ... + \theta_q^2}$ for q

Or

0 for k > q


## Autoregressive Processes

$Y_t = \phi_1Y_{t-1} +  \phi_2Y_{t-2} + ... + \phi_{\rho}Y_{t-\rho} + e_t$

### First Order AR(1)

$Y_t = \phi Y_{t-1} + e_t$

\[
\begin{eqnarray}
\gamma_o & = & Var(Y_t) \\
& = & Var(\phi Y_{t-1 + e_t}) \\
& = & Var(\phi Y_{t-1}) + Var(e_t) \\
& = & \phi^2 Var(Y_{t-1} + \sigma_e^2) \\
& = & \phi^2 \gamma_o + \sigma_e^2 \\
& = & \phi^2 \gamma_o + \sigma_e^2 \\
& = & \gamma_o - \phi^2 \\
& = & \sigma_e^2 \\
& = & \gamma_o(1-\phi^2) \\
& = & \sigma_e^2 \\
& = & \gamma_o  \\ 
& = & \frac{\sigma_e^2}{1-\phi^2} \\
\end{eqnarray}
\]

**Expected Value**

\[
\begin{eqnarray}
E(Y_{t-k}Y_t) & = & E(\phi Y_{t-1}Y_{t-k}) + E(e_tY_{t-k}) \\
\gamma_k & = & \phi\gamma_{k-1}
\end{eqnarray}
\]

for k = 1, 2, ...

So, $\rho_k = \frac{\gamma_k}{\gamma_o} = \phi^k$

  * if $\rho \leq 1$ and you see an exponential decay on the acf plot, think AR model
  * if $\rho$ is negative, the acf plot will oscilate back and forth between positive and negative values
  

#### AR(1) Recursion

Let $Y_t = \phi Y_{t-1} + e_t$ as before, but $Y_t = \phi Y_{t-2} + e_{t-1}$

So $Y_t = \phi (\phi Y_{t-2} + e_{t-1} + e_t) = e_t + \phi e_{t-1} + \phi^2 Y_{t-2}$

Then $Y_{t-2} = \phi Y_{t-3} + e_{t-2}$ and so on .....

Assume $\lvert \phi \rvert < 1$

Notice, $Y_t = e_t + \phi e_{t-1} + \phi^2 e_{t-2} + \phi^3 e_{t-3} + ...$ is an infinite series.

**Expected Value**

Then $E(Y_t) = E[e_t + \phi e_{t-1} + \phi^2 e_{t-2} + ...] = 0 + 0 + 0 + ...$ since the expected value of $e_t$ is 0.

**Variance**

The $Var(Y_t) = Var[e_t + \phi e_{t-1} + \phi^2 e_{t-2} + \phi^3 e_{t-3} + ...$, do a little algebra (look in the book) and the $Var(Y_t) = \frac{\sigma_e^2}{1-\phi^2}$

**Covariance**

$Cov(Y_t, Y_{t-1})$, do some algebra so expand then simplify, and the $Cov(Y_t, Y_{t-1} = \phi \sigma_e^2(1+\phi^2 + \phi^4 + \phi^6+ ...) = \frac{\phi \sigma_e^2}{1-\phi^2}$

So, $\rho_1 = \frac{\gamma_1}{\gamma_o} = \frac{cov(Y_t, Y_{t-1}}{cov(Y_t, Y_t)} = \phi$

$\rho_1$ should be a good estimator for $\phi$.

Recall, $\rho_k = \phi^k$

**Any Lag Value of k**

$cov(Y_t, Y_{t-k}) = \frac{\phi^k \sigma_e^2}{1-\phi^2}$

Simplified, $\rho_k = \phi^k$

### Second Order AR(2) Process

$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + e_t$

$\phi_1$ and $\phi_2$ are the parameters

$e_t$ terms are a zero mean, constant variance white-noise process where the $e_t$ is independent of the past $Y_{t-1}, Y_{t-2}, Y_{t-3}, ...$

Define the AR characteristic polynomial as $\phi (X) = 1 - \phi_1 X - \phi_2 X^2$.

The AR characteristics equation is $\phi (X) = 0$ or $1 - \phi_1X_1 - \phi_2 X_2 = 0$.

The equation will always have two roots (possibly complex solution).

Let $Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + e_t$ where $e_t$ is independendt of $Y_{t-1}, Y_{t-2}, ...$

*Aside:* Define B as the "backshift" operator, so $BY_t = Y_{t-1}$ and $B^2Y_t = Y_{t-2}$

Then $Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + e_t = \phi_1 BY_t + \phi_2 B^2Y_t + e_t$

Do a little algebra and $e_t = (1-\phi_1 B-\phi_2 B^2)Y_t$ **(known as the characteristic polynomial)

**Roots of the Characteristic Equation** 

$\frac{\phi_1 \pm \sqrt{\phi_1 + 4\phi_2}}{-2\phi_2}$

**Stationarity Conditions**

  * $\phi_1 + \phi_2 < 1$
  * $\phi_2 - \phi_1 < 1$
  * $\lvert \phi_2 \rvert < 1$

$Y_t$ is a stationary series if and onlyl if the characteristic equation exceeds 1 in absolute value (or modulus)

**Yule-Walker Equation**

$\gamma_k = cov(Y_t, Y_{t-k}) = \phi_1 \gamma_{k-1} + \phi_2 \gamma_{k-2}$ for k = 1, 2, ...

$\rho_k = \frac{\gamma_k}{\gamma_o} = \phi_1 \rho_k-1 + \phi_2 \rho_{k-2}$ for k = 1, 2, ...

$\rho_o = 1$

$\rho_1 = \phi_1 \rho_1 + \phi_2 \rho_2$ do algebra and substitution and 

$\rho_1 = \frac{\phi_1}{1-\phi_2}$

$\rho_2 = \frac{\phi_1^2}{1-\phi_2} + \phi_2$


#### Another Approach

Let the roots of the characteristic equation be $\frac{1}{G_1}$ and $\frac{1}{G_2}$.  *See page 84 for the algebra*

If 
$G_1 \ne G_2$
then,

$\rho_k = \frac{(1-G_2^2)G_1^{k+1} - (1-G_1^2)G_2^{k+1}}{(G_1 - G_2)(1+G_1G_2)}$

If 
$G_1 = G_2$ 
then,

$\phi_k = [1+K(\frac{1+\phi_2}{1+\phi_2})][\frac{\phi_1}{2}]^k$

If 
$\frac{1}{G_1}$ and $\frac{1}{G_2}$
are complex,

$\phi_k = R^k \frac{sin(\Theta k + \phi)}{sin(\phi)}$

#### In Class Example

$\rho_0 = 1$

$\rho_1 = \frac{\phi_1}{1- \phi_2} = \frac{0.5}{1-(-.5)} = \frac{1}{3}$

\[
\begin{eqnarray}
\rho_2 & = & \phi_1 \rho_1 + \phi_2 \rho_0 \\
& = & \frac{1}{2}(\frac{1}{3}) - (-\frac{1}{2})(1) \\
& = & -\frac{1}{3}
\end{eqnarray}
\]

*We looked at acf plot of AR(2) model*


## ARMA (1, 1) Process

  * Combination of Autoregressive and moving average process
  
AR(1) looks like:

$Y_t = \phi Y_{t-1} + e_t$

Let B be the backshift operator:
$e_t = Y_t - \phi Y_{t-1}$

$e_t = (1-\phi B)Y_t$

$1-\phi x = 0$

$x = \frac{1}{\phi}$

MA(1) looks like:

$Y_t = e_t - \theta e_{t-1}$

$ = (1-\phi B)e_t$

***

ARMA (1, 1) looks like:

$Y_t = \phi Y_{t-1} - \theta e_{t-1} + e_t$

**Expected Value**

$E(e_tY_t) = E[e_t(\phi Y_{t-1} - \phi e_t + e_t)]$

$E(e_tY_t) = E[\phi e_tY_{t-1} - \theta e_t e_{t-1} + e_te_t]$

$E(e_tY_t) = \phi E[e_tY_{t-1} - \theta E(e_t)(e_{t-1}) + E(e_te_t)]$

$E(e_tY_t) = 0 - 0 + E(e_t^2) = \sigma_e^2$

***

$E(e_{t-1}Y_t) = E[e_{t-1}(\phi Y_{t-1} - \theta e_{t-1} + e_t)]$

$E(e_{t-1}Y_t) = \phi \sigma_e^2 - \theta \sigma_e^2$

(4.4.3) in the book

$\gamma_o = \phi \gamma_1 + [1-\phi(\theta - \phi)]\sigma^2$

$\gamma_1 = \phi \gamma_o - \theta \sigma_e^2$

Simplify.....

$\gamma_o = \frac{1-2\phi \theta + \theta^2}{1-\theta^2}\sigma_e^2$

Then rho equals

$\rho_k = \frac{(1-\theta \phi)(\phi - \theta)}{1-2\theta \phi + \theta^2}\phi^{k-1}$ for $k \geq 1$

 * ACF will follow these values
 
## Invertibility

Consider the MA(1) model

$Y_t = e_t - \theta e_{t-1}$

Solve for $e_t$

\[
\begin{eqnarray}
e_t & = & Y_t + \theta e_{t-1} \\
& = & Y_t + \theta[Y_{t-1} + \theta e_{t-2}] \\
& = & Y_t + \theta Y_{t-1} + \theta^2[e_{t-2}] \\
& = & Y_t + \theta Y_{t-1} + \theta^2 Y_{t-2} + \theta^3 e_{t-3} \\
& = & Y_t + \theta Y_{t-1} + \theta^2 Y_{t-2} + \theta^3 Y_{t-3} + \theta^4 Y_{t-4} + ... \\
\end{eqnarray}
\]

Solve for $Y_t$

$Y_t = e_t - \theta Y_{t-1} - \theta^2 Y_{t-2} - \theta^3 Y_{t-3} - ... $

  * Infinite AR process
  * To be meaningful we need $\sum_{j=1}^{\infty} \theta^j < \infty$ which occurs if $\lvert \theta \rvert < 1$
  * A process $Y_t$ is invertible if it can be written as a mathematically meaningful AR process
  * Notice $\lvert \theta \rvert < 1$ is the "invertibility" condition for the MA(1) process.  This corresponds to the stationarity requirement of $\lvert \theta \rvert < 1$ in the AR(1) model.
  
$Y_t = e_t - \theta e_{t-1}$

$Y_t = e_t - \frac{1}{\theta}e_{t-1}$

ACF is the same.  To obtain a unique solution, we require $\lvert \theta \rvert < 1$.

Notice for $Y_t = e_t - \frac{1}{\theta}Y_{t-1} - (\frac{1}{\theta})^2Y_{t-2} - (\frac{1}{\theta})^3Y_{t-3} - ...$

This is no longer meaningful since $\sum_{j=1}^{\infty} (\frac{1}{\theta})^j$ diverges.

**Backshift Notation**

$Y_t = (1-\theta B)e_t$

$Y_t = e_t - \theta Be_t$

$Y_t = e_t - \theta e_{t-1}$

MA characteristic function then $1 - \theta x = 0$ solve to get $x = \frac{1}{\theta}$.  Roots exceed 1 $\lvert \frac{1}{\theta} > 1$ implies $\lvert \theta \rvert < 1$.

**Summary**

For any AR(p) process to be stationary, the roots of the AR characteristic polynomial $\phi(x) = 1 - \phi_1x - \phi_2x^2 - \phi_3x^3 - ... - - \phi_px^p$ all exceed 1 (in modulus or absolute value)

  * For any MA(q) process to be invertible the roots of the MA characteristic polynomial  $\theta (x) = 1 - \theta_1 x - \theta_2 x^2 - \theta_3 x^3 - ... - 1- \theta_q x^q$ all exceed 1 in absolute value (or modulus)
  * All invertible processes are stationary
  * All stationary AR processes are invertible
  * Any invertible MA(q) process corresponds to an infinite order AR process
  * Any stationary AR(p) process corresponds to an infinite order MA process
  
###In Class Examples



```{r ma_process}
e505n = rnorm(505, 0, 1) #n = 102, mean = 0, st. dev. = 1
theta1<- 0.5
ytma1arinf<-1:500

for (i in 1:500) {
  ytma1arinf[i]= e505n[i+2]-theta1*e505n[i+1]
}
plot(ytma1arinf, ylab='MA1', xlab='Time', type='o')
title('MA(1) Theta1 = 0.5')

acf(ytma1arinf)

```


```{r ar_process}
arinf<-1:500
arinf[1]<-e505n[1]
arinf[2]<-e505n[2]-theta1*arinf[1]
arinf[3]<-e505n[3]-theta1^2*arinf[1]-theta1*arinf[2]
arinf[4]<-e505n[4]-theta1^3*arinf[1]-theta1^2*arinf[2]-theta1*arinf[3]
arinf[5]<-e505n[5]-theta1^4*arinf[1]-theta1^3*arinf[2]-theta1^2*arinf[3]-theta1*arinf[4]
for (i in 6:500) {
  arinf[i]= e505n[i]-theta1*arinf[i-1]-theta1^2*arinf[i-2]-theta1^3*arinf[i-3]-theta1^4*arinf[i-4]
  }
plot(arinf[10:500], ylab='AR4 for MA1', xlab='Time', type='o')
title('MA(1) Theta1 = 0.5')

acf(arinf)

```

